{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bigearthnet_encoder.encoder import tiff_dir_to_ben_s2_patch\n",
    "from bigearthnet_common.example_data import get_s2_example_folder_path\n",
    "from bigearthnet_common.base import get_s2_patch_directories\n",
    "\n",
    "s2_path = get_s2_example_folder_path()\n",
    "example_patch = [p for p in s2_path.iterdir()][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Path('/home/kaiclasen/.local/share/bigearthnet_common/BigEarthNet-S2-Example/S2A_MSIL2A_20170617T113321_4_55'),\n",
       " Path('/home/kaiclasen/.local/share/bigearthnet_common/BigEarthNet-S2-Example/S2A_MSIL2A_20170617T113321_36_85'),\n",
       " Path('/home/kaiclasen/.local/share/bigearthnet_common/BigEarthNet-S2-Example/S2A_MSIL2A_20171221T112501_56_35')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s2_patch_paths = get_s2_patch_directories(s2_path)\n",
    "s2_patch_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  7.83it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "from bigearthnet_encoder.encoder import write_S2_squirrel\n",
    "from bigearthnet_encoder.squirrel_ext import ConfigurableMessagePackDriver\n",
    "\n",
    "write_S2_squirrel(\n",
    "    s2_path,\n",
    "    \"/tmp/delete_me\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bigearthnet_encoder.squirrel_ext.ConfigurableMessagePackDriver at 0x7f32a41517c0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msgpack_driver = ConfigurableMessagePackDriver(\"/tmp/delete_me/\")\n",
    "msgpack_driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(msgpack_driver.keys())\n",
    "v = msgpack_driver.get_iter()\n",
    "data = list(v)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object supporting the buffer API required",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/kaiclasen/bigearthnet_encoder/notebooks/squirrel.ipynb Cell 17'\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bmars_alba/home/kaiclasen/bigearthnet_encoder/notebooks/squirrel.ipynb#ch0000045vscode-remote?line=8'>9</a>\u001b[0m s \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhello_world\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmars_alba/home/kaiclasen/bigearthnet_encoder/notebooks/squirrel.ipynb#ch0000045vscode-remote?line=9'>10</a>\u001b[0m \u001b[39m# hashlib.md5(s.encode(encoding=\"UTF-8\")).hexdigest()\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bmars_alba/home/kaiclasen/bigearthnet_encoder/notebooks/squirrel.ipynb#ch0000045vscode-remote?line=10'>11</a>\u001b[0m hashlib\u001b[39m.\u001b[39;49mmd5(data)\n",
      "\u001b[0;31mTypeError\u001b[0m: object supporting the buffer API required"
     ]
    }
   ],
   "source": [
    "import hashlib\n",
    "\n",
    "# for alg in hashlib.algorithms_available:\n",
    "#     try:\n",
    "#         print(alg, hashlib.new(alg).digest_size)\n",
    "#     except:\n",
    "#         pass\n",
    "\n",
    "s = \"hello_world\"\n",
    "# hashlib.md5(s.encode(encoding=\"UTF-8\")).hexdigest()\n",
    "hashlib.md5(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.898550724637681"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "80 / 23 * 50 / 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashlib.md5()\n",
    "# s2_patch = tiff_dir_to_ben_s2_patch(example_patch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compare performance impact between only storing the numpy arrays by their spatial resolution\n",
    "# vs storing each band individually\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "161607"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "# 160KB per patch with all bands\n",
    "patch_size_in_bytes = sys.getsizeof(s2_patch.dumps())\n",
    "patch_size_in_bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6644"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2**30 // patch_size_in_bytes\n",
    "# around 6600 patches per GB\n",
    "# 80~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lmdb\n",
    "from rich.progress import Progress\n",
    "from bigearthnet_patch_interface.s2_interface import BigEarthNet_S2_Patch\n",
    "import bigearthnet_common.constants as ben_constants\n",
    "from typing import List\n",
    "\n",
    "SHARD_SIZE = 6600\n",
    "BATCH_SIZE = 16\n",
    "REPS = 2\n",
    "TOTAL_TEST_SIZE = SHARD_SIZE * REPS\n",
    "\n",
    "\n",
    "def interpolate_to_ben_perf(size, seconds):\n",
    "    interpolated_seconds = ben_constants.BEN_COMPLETE_SIZE / size * seconds\n",
    "    return f\"Would take {interpolated_seconds / 60:.02} min to pass through BigEarthNet\"\n",
    "\n",
    "\n",
    "def fake_lmdb_builder(fake_data, keys: List[str], lmdb_path: str = \"S2_lmdb.db\"):\n",
    "    max_size = 2**40  # 1TebiByte\n",
    "    env = lmdb.open(str(lmdb_path), map_size=max_size, readonly=False)\n",
    "\n",
    "    with Progress() as progress:\n",
    "        task = progress.add_task(\"Building LMDB archive\", total=len(keys))\n",
    "        for key in keys:\n",
    "            with env.begin(write=True) as txn:\n",
    "                txn.put(key.encode(\"utf-8\"), fake_data)\n",
    "                progress.update(task, advance=1)\n",
    "        env.close()\n",
    "\n",
    "\n",
    "keys = [f\"{i:05}\" for i in range(TOTAL_TEST_SIZE)]\n",
    "# generate fake data and keys\n",
    "# fake_lmdb_builder(s2_patch.dumps(), keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_fake_lmdb(keys, lmdb_path=\"S2_lmdb.db\"):\n",
    "    # readahead should be True if dataset fits in RAM\n",
    "    # otherwise it may be faster to set readahead = False\n",
    "    # as readonly=True no need for `locking` which _should_ take longer if lock=True\n",
    "    env = lmdb.open(str(lmdb_path), readonly=True, readahead=True, lock=False)\n",
    "    # possible optimization use single call to\n",
    "    # getmulti(keys) instead of a new thread with a single element as transaction?\n",
    "\n",
    "    for key in keys:\n",
    "        with env.begin() as txn:\n",
    "            byteflow = txn.get(key.encode(\"utf-8\"))\n",
    "            s2_patch = BigEarthNet_S2_Patch.loads(byteflow)\n",
    "\n",
    "\n",
    "# ~4 sek to pass through 6600 * 2\n",
    "# ~1.2sek to .loads\n",
    "read_fake_lmdb(keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Would take 1.8e+02 min to pass through BigEarthNet'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# interpolate_to_ben_perf(TOTAL_TEST_SIZE, 1.2)\n",
    "interpolate_to_ben_perf(6600, 120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "from pathlib import Path\n",
    "from bigearthnet_encoder.squirrel_ext import (\n",
    "    _patch_interface_to_dict,\n",
    ")\n",
    "from squirrel.iterstream import IterableSource\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MyMessagePackDriver' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/kaiclasen/bigearthnet_encoder/notebooks/squirrel.ipynb Cell 11'\u001b[0m in \u001b[0;36m<cell line: 25>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmars_alba/home/kaiclasen/bigearthnet_encoder/notebooks/squirrel.ipynb#ch0000015vscode-remote?line=21'>22</a>\u001b[0m \u001b[39m# p = Path(SAVE_URL)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmars_alba/home/kaiclasen/bigearthnet_encoder/notebooks/squirrel.ipynb#ch0000015vscode-remote?line=22'>23</a>\u001b[0m \u001b[39m# shutil.rmtree(p)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmars_alba/home/kaiclasen/bigearthnet_encoder/notebooks/squirrel.ipynb#ch0000015vscode-remote?line=23'>24</a>\u001b[0m url \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m../Ben_S2_None_Compression\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bmars_alba/home/kaiclasen/bigearthnet_encoder/notebooks/squirrel.ipynb#ch0000015vscode-remote?line=24'>25</a>\u001b[0m msgpack_driver \u001b[39m=\u001b[39m MyMessagePackDriver(url)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'MyMessagePackDriver' is not defined"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "from pathlib import Path\n",
    "from bigearthnet_encoder.squirrel_ext import (\n",
    "    _patch_interface_to_dict,\n",
    "    _write_s2_msgpack\n",
    "    \n",
    ")\n",
    "from squirrel.iterstream import IterableSource\n",
    "\n",
    "\n",
    "def gen_shards():\n",
    "    while True:\n",
    "        yield s2_patch_dict\n",
    "\n",
    "\n",
    "it = IterableSource(iter(gen_shards()))\n",
    "url = \"dummy\"\n",
    "# Other supported compressions:\n",
    "# fsspec.compression.available_compressions()\n",
    "# [None, 'zip', 'bz2', 'gzip', 'lzma', 'xz']\n",
    "compression = \"None\"\n",
    "SAVE_URL = f\"{url}_{compression}\"\n",
    "# p = Path(SAVE_URL)\n",
    "# shutil.rmtree(p)\n",
    "url = \"../Ben_S2_None_Compression\"\n",
    "msgpack_driver = MyMessagePackDriver(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  9.22it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "from bigearthnet_common.constants import Split\n",
    "from bigearthnet_common.base import get_original_split_from_patch_name\n",
    "from bigearthnet_common.base import get_s2_patch_directories\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2.5 * B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(msgpack_driver.keys())\n",
    "# 57s for normal threaded iterator\n",
    "# >2min for ppool with 12 workers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bigearthnet_common.sets import get_recommended_s2_patches, filter_patches_by_split, get_all_s2_patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "valids = get_recommended_s2_patches()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "valids_train = filter_patches_by_split(\"S2\", valids, \"train\")\n",
    "s2_patches = get_all_s2_patches()\n",
    "all_train = filter_patches_by_split(\"S2\", s2_patches, \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only async_map without anything else and calling b1_mean: 58s\n",
    "# only async_map without anything else and lambda: 1m 4s\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from numba import njit\n",
    "\n",
    "ppool = ProcessPoolExecutor(max_workers=8)\n",
    "\n",
    "bands = \"B01 B02 B03 B04 B05 B06 B07 B08 B8A B09 B11 B12\".split(\" \")\n",
    "\n",
    "def b1(v):\n",
    "    return v[\"B01\"]\n",
    "\n",
    "def mean(v):\n",
    "    return v.mean()\n",
    "\n",
    "def b1_mean(v):\n",
    "    return mean(b1(v))\n",
    "\n",
    "def val_train(v):\n",
    "    return v[\"patch_name\"] in valids_train\n",
    "\n",
    "def in_all_train(v):\n",
    "    return v[\"patch_name\"] in all_train\n",
    "\n",
    "def bands_maxes(v):\n",
    "    return {b: v[b].max() for b in bands}\n",
    "\n",
    "def bands_mines(v):\n",
    "    return {b: v[b].min() for b in bands}\n",
    "\n",
    "def bands_means(v):\n",
    "    return {b: v[b].mean() for b in bands}\n",
    "\n",
    "@njit(parallel=True)\n",
    "def _numba_mean(a):\n",
    "    return a.mean()\n",
    "\n",
    "\n",
    "def numba_mean_b1(v):\n",
    "    return _numba_mean(v[\"B01\"])\n",
    "\n",
    "# means = msgpack_driver.get_iter().filter(in_all_train).async_map(b1_mean, executor=ppool).collect()\n",
    "\n",
    "# currently spawns 4178 threads... Requires 25% cpu utilization\n",
    "# takes 1min25s\n",
    "# means = msgpack_driver.get_iter().filter(in_all_train).async_map(numba_mean_b1).collect()\n",
    "\n",
    "# with limiting the number of workers to 8 \n",
    "# takes 1m19s\n",
    "# produces \"only\" 1105 threads\n",
    "# means = msgpack_driver.get_iter().filter(in_all_train).async_map(numba_mean_b1, max_workers=8).collect()\n",
    "\n",
    "# Without numba call stack and 8 workers\n",
    "# 88 threads and almost no CPU utilization\n",
    "# 44s\n",
    "# means = msgpack_driver.get_iter().filter(in_all_train).async_map(b1_mean, max_workers=8).collect()\n",
    "\n",
    "# b1 mean with ProcessPool and 8 workers takes: 3m02s only train\n",
    "# all_mins = msgpack_driver.get_iter().async_map(bands_means).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only train now 40s\n",
    "# means = msgpack_driver.get_iter().filter(in_all_train).async_map(lambda v: v[\"patch_name\"]).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in msgpack_driver.get_iter():\n",
    "    # print(i)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.functional import interpolate\n",
    "from squirrel.iterstream.torch_composables import TorchIterable\n",
    "import numpy as np\n",
    "from bigearthnet_common.constants import BEN_10m_CHANNELS, BEN_20m_CHANNELS\n",
    "import torch\n",
    "\n",
    "def interp_patches(patch_dict):\n",
    "    bands_10m = np.stack([patch_dict[b] for b in BEN_10m_CHANNELS])\n",
    "    bands_10m_torch = Tensor(np.float32(bands_10m)).unsqueeze(dim=0)\n",
    "\n",
    "    bands_20m = np.stack([patch_dict[b] for b in BEN_20m_CHANNELS])\n",
    "    bands_20m_torch = Tensor(np.float32(bands_20m)).unsqueeze(dim=0)\n",
    "\n",
    "    bands_20m_interp = interpolate(\n",
    "        bands_20m_torch,\n",
    "        bands_10m.shape[-2:],\n",
    "        mode=\"bicubic\",\n",
    "    )\n",
    "    # print(bands_10m_torch.shape, bands_20m_interp.shape)\n",
    "    tns = torch.concat((bands_10m_torch.squeeze(), bands_20m_interp.squeeze()), axis=0)\n",
    "    # This is my magic normalization value to push it to the 0--1 range\n",
    "    return tns / 10_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "import kornia.augmentation as K\n",
    "from squirrel.iterstream.torch_composables import SplitByWorker\n",
    "import pytorch_lightning as pl\n",
    "from torch.optim import Adam\n",
    "\n",
    "# WIP!\n",
    "class FastModel(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model= timm.create_model(\"efficientnet_b0\", pretrained=False, in_chans=10)\n",
    "        self.criterion = torch.nn.BCEWithLogitsLoss(reduction='mean')\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return Adam(self.parameters(), lr=1e-3)\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        it = (\n",
    "            msgpack_driver.get_iter(key_hooks=[SplitByWorker]).filter(in_all_train).async_map(interp_patches, max_workers=16).compose(TorchIterable)\n",
    "        )\n",
    "        dl = DataLoader(it, batch_size=1024, pin_memory=True, num_workers=4)\n",
    "        return dl\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        inputs, targets = batch\n",
    "        outputs = self.model(inputs)\n",
    "        loss = self.criterion(outputs, targets)\n",
    "        return loss\n",
    "\n",
    "augs = K.AugmentationSequential(\n",
    "    K.RandomHorizontalFlip(),\n",
    "    K.RandomVerticalFlip(),\n",
    "    K.RandomResizedCrop(size=(120, 120)),\n",
    "    # Setting this to true has a very limited performance impact\n",
    "    # at least for a batch size of 1024\n",
    "    same_on_batch=False,\n",
    ")\n",
    "# testing with 8 workers\n",
    "# batch size 128\n",
    "    # 10% CPU utilization with 656 threads\n",
    "    # 2.7GB GPU memory 4% volatile memory\n",
    "    # 4m30s for running through dataset on CUDA\n",
    "    # dl = DataLoader(pipe, batch_size=128, pin_memory=True, num_workers=2)\n",
    "\n",
    "    # with 2 workers:\n",
    "    # CPU: 15% (not so sure...)\n",
    "    # 2.7GB GPU memory but 21% volatile GPU-Util\n",
    "    # only 1m31 sek :o\n",
    "\n",
    "    # with 4 workers:\n",
    "    # CPU: ~20%\n",
    "    # 26% volatile GPU-util\n",
    "    # only 1m10s for the entire pass with 4 workers...\n",
    "\n",
    "# batch size 1024\n",
    "# with 4 workers\n",
    "    # CPU: 13%\n",
    "    # VRAM: 6GB \n",
    "    # 1min\n",
    "\n",
    "# batch size 10_000\n",
    "# with 4 workers\n",
    "    # CPU: <10%\n",
    "    # VRAM: 10GB \n",
    "    # ~1m23s\n",
    "\n",
    "# Testing with 16 max_workers for async_map patch interp\n",
    "    # ~1m10s\n",
    "    # smaller prefetch buffer had no effect\n",
    "    # larger prefetch buffer increased RAM usage; nothing really changed the speed\n",
    "\n",
    "\n",
    "# Applying augmentation on the CPU:\n",
    "    # 30% CPU utilization\n",
    "    # may spike up to 40%\n",
    "    # takes 3m4s for CPU only with 10_024\n",
    "    # could be different if real model is used with smaller batch size\n",
    "\n",
    "# Nice! shuffle has almost no effect on the processing time!\n",
    "# 1m10s\n",
    "pipe = msgpack_driver.get_iter(key_hooks=[SplitByWorker]).filter(in_all_train).shuffle(10_000).async_map(interp_patches, max_workers=16).compose(TorchIterable)\n",
    "# increasing number of workers for dataloader doesn't do a lot for my purposes\n",
    "\n",
    "# storing data as list in memory is _not_ faster!\n",
    "# reading from list with a single worker is actually slower!\n",
    "# 1m26s\n",
    "# even with setting the num_workers=2 the compute time is not significantly faster!\n",
    "# it is even slower! Maybe due to the extra copying?\n",
    "# >2m.15\n",
    "# l = list(pipe)\n",
    "\n",
    "dl = DataLoader(pipe, batch_size=1024, num_workers=2, pin_memory=True)\n",
    "device = torch.device(\"cuda:0\")\n",
    "for _ in range(1):\n",
    "    for b in dl:\n",
    "        data = augs(b.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = DataLoader(pipe, batch_size=1024, pin_memory=True, num_workers=4)\n",
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "# TODO: Actually test with a 'real' model!\n",
    "\n",
    "for _ in range(1):\n",
    "    for b in dl:\n",
    "        data = augs(b.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kaiclasen/.cache/pypoetry/virtualenvs/bigearthnet-encoder-sA-plAN6-py3.8/lib/python3.8/site-packages/torch/cuda/__init__.py:82: UserWarning: CUDA initialization: Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 802: system not yet initialized (Triggered internally at  ../c10/cuda/CUDAFunctions.cpp:112.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 10, 120, 120])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "augs(b).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# async_map probably destroys the async_map torch.mean calculation\n",
    "# 4m 35s for generating the \n",
    "means = msgpack_driver.get_iter().filter(in_all_train).async_map(interp_patches, max_workers=12).async_map(torch.mean).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for b in bands:\n",
    "#     m = min(v[b] for v in all_mins)\n",
    "#     print(b, \" min: \", m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(means) == len(all_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "352.75488109716525"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# means of b1 mean: 711.450056265521\n",
    "np.mean(means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean': {'B01': 340.76769064,\n",
       "  'B02': 429.9430203,\n",
       "  'B03': 614.21682446,\n",
       "  'B04': 590.23569706,\n",
       "  'B05': 950.68368468,\n",
       "  'B06': 1792.46290469,\n",
       "  'B07': 2075.46795189,\n",
       "  'B08': 2218.94553375,\n",
       "  'B8A': 2266.46036911,\n",
       "  'B09': 2246.0605464,\n",
       "  'B11': 1594.42694882,\n",
       "  'B12': 1009.32729131},\n",
       " 'std': {'B01': 554.81258967,\n",
       "  'B02': 572.41639287,\n",
       "  'B03': 582.87945694,\n",
       "  'B04': 675.88746967,\n",
       "  'B05': 729.89827633,\n",
       "  'B06': 1096.01480586,\n",
       "  'B07': 1273.45393088,\n",
       "  'B08': 1365.45589904,\n",
       "  'B8A': 1356.13789355,\n",
       "  'B09': 1302.3292881,\n",
       "  'B11': 1079.19066363,\n",
       "  'B12': 818.86747235}}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ben_constants.BAND_STATS_S2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bigearthnet_common.constants as ben_constants\n",
    "assert len(names) == ben_constants.BEN_COMPLETE_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bigearthnet_encoder.squirrel_ext import _write_s2_msgpack\n",
    "from bigearthnet_common.example_data import get_s2_example_folder_path\n",
    "from bigearthnet_common.base import get_s2_patch_directories\n",
    "from squirrel.iterstream import IterableSource\n",
    "from pathlib import Path\n",
    "\n",
    "# s2_path = get_s2_example_folder_path()\n",
    "s2_path = Path(\"~/datasets/BigEarthNet-v1.0/BigEarthNet-v1.0\").expanduser()\n",
    "patch_paths = get_s2_patch_directories(s2_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_paths = patch_paths[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_write_s2_msgpack(patch_paths, \"dummy_None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(msgpack_driver.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpolate_to_ben_perf(6600, 60 * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from functools import partial\n",
    "# to_shard = partial(msgpack_driver.store.set, compression=None)\n",
    "# batches = it.take(TOTAL_TEST_SIZE).batched(SHARD_SIZE, drop_last_if_not_full=False).map(to_shard).join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GZIP: 6.5s for pass-through\n",
    "# ~797MB\n",
    "# 6.8\n",
    "# with not prefetch_buffer 6.8s\n",
    "\n",
    "# read using the messagepack driver\n",
    "\n",
    "it_msg_pack = msgpack_driver.get_iter()\n",
    "for item in it_msg_pack.take(10):\n",
    "    # np.mean(item[\"10m_bands\"])\n",
    "    print(item[\"patch_name\"])\n",
    "    print(item[\"B01\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpolate_to_ben_perf(TOTAL_TEST_SIZE, 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('bigearthnet-encoder-sA-plAN6-py3.8': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "29428d7ce421736645eac04ea70a3413592fd28396d481e85ce1d30bb18d59f8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
